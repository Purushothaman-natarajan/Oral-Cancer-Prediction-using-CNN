{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Author : Purushothaman Natarajan\n",
        "# Title : Cancer prediction using CNN (transfer learning)\n",
        "# Dataset : Open source dataset found in Kaggle (Oral Cancer)"
      ],
      "metadata": {
        "id": "gzqNQRfB_nYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "v38w0fpR_hKi"
      },
      "outputs": [],
      "source": [
        "# Install necessary Libraries\n",
        "!pip install tensorflow\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tabulate\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JL4p4PI_hKj"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r-z7s5U_hKj"
      },
      "outputs": [],
      "source": [
        "#print the current directory\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA9Le_1T_hKk"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "labels = []\n",
        "\n",
        "master_data_path=\"/tf/red-model/C_Devel/unzipps/Cancer Prediction\"\n",
        "\n",
        "def load_images_from_folder(folder_path, label):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if os.path.isdir(file_path):\n",
        "            # If the item is a directory, recursively load images from it\n",
        "            load_images_from_folder(file_path, filename)\n",
        "        elif filename.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "            # If the item is an image file, load it and add it to the images list\n",
        "            try:\n",
        "                img = Image.open(file_path)\n",
        "                images.append(img)\n",
        "                labels.append(label)\n",
        "            except Exception as e:\n",
        "                print(f'Error loading image {file_path}: {str(e)}')\n",
        "        else:\n",
        "            print(f'Skipping non-image file: {file_path}')\n",
        "\n",
        "# Iterate through folders in the master folder\n",
        "for folder_name in os.listdir(master_data_path):\n",
        "    folder_path = os.path.join(master_data_path, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        # Load images and labels from the current folder\n",
        "        load_images_from_folder(folder_path, folder_name)\n",
        "\n",
        "print(f'Images loaded: {len(images)}')\n",
        "print(f'Labels loaded: {len(labels)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-55drO6y_hKk"
      },
      "outputs": [],
      "source": [
        "#counting the number of labels in each classes\n",
        "\n",
        "count_of_classes = {}\n",
        "\n",
        "for label in labels:\n",
        "    if label in count_of_classes:\n",
        "        count_of_classes[label] +=1\n",
        "    else:\n",
        "        count_of_classes[label] = 1\n",
        "\n",
        "\n",
        "for key, value in count_of_classes.items():\n",
        "    print(f'{key}:{value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxDNHceK_hKk"
      },
      "outputs": [],
      "source": [
        "dimension = []\n",
        "for idx, image in enumerate(images):\n",
        "    width, height = image.size\n",
        "    current_dimension = (width, height)\n",
        "    dimension.append(current_dimension)\n",
        "\n",
        "unique_dimension = list(set(dimension))\n",
        "print(f'Unique dimension in the dataset: {len(unique_dimension)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofIePDAU_hKk"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGmGmU8Z_hKl"
      },
      "outputs": [],
      "source": [
        "# Convert PIL images to numpy arrays\n",
        "numpy_images = [np.array(image) for image in images]\n",
        "\n",
        "# Resize and convert images to RGB format if necessary\n",
        "target_size = (224, 224)\n",
        "reshaped_images = []\n",
        "for idx, image in enumerate(numpy_images):\n",
        "    pil_image = Image.fromarray(image)\n",
        "    pil_image = pil_image.resize(target_size)\n",
        "    # Convert to RGB if image is grayscale\n",
        "    if pil_image.mode != 'RGB':\n",
        "        pil_image = pil_image.convert('RGB')\n",
        "    reshaped_images.append(np.array(pil_image))\n",
        "\n",
        "\n",
        "print(\"Dimension of reshaped_images:\", reshaped_images[0].shape)\n",
        "\n",
        "print(\"Data type of reshaped_images:\", reshaped_images[0].dtype)\n",
        "\n",
        "\n",
        "# Convert images to float32 and normalize to [0, 1]\n",
        "normalized_images = np.array(reshaped_images, dtype=np.float32) / 255.0\n",
        "\n",
        "# Verify the shape and data type of processed_images\n",
        "print(\"Shape of normalized_images:\", normalized_images.shape)\n",
        "print(\"Data type of processed_images:\", normalized_images.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HsEi2bK_hKl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED9kbvuF_hKl"
      },
      "outputs": [],
      "source": [
        "print(labels[0])\n",
        "plt.imshow(normalized_images[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGw_yk6F_hKl"
      },
      "source": [
        "## Handling the imbalance in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl95FGzU_hKm"
      },
      "outputs": [],
      "source": [
        "#counting the number of labels in each classes\n",
        "\n",
        "count_of_classes = {}\n",
        "\n",
        "for label in labels:\n",
        "    if label in count_of_classes:\n",
        "        count_of_classes[label] +=1\n",
        "    else:\n",
        "        count_of_classes[label] = 1\n",
        "\n",
        "\n",
        "for key, value in count_of_classes.items():\n",
        "    print(f'{key}:{value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg7qiHzg_hKm"
      },
      "outputs": [],
      "source": [
        "required_size = round(len(labels)/2)\n",
        "print(f\"we should have {required_size} images in each class\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Yv_T_eM_hKn"
      },
      "outputs": [],
      "source": [
        "set(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeIH7MHP_hKn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of synthetic images you want to generate for each class\n",
        "num_synthetic_images_per_class = 638\n",
        "\n",
        "# Define a function to apply augmentations and generate synthetic images\n",
        "def generate_synthetic_image(image_data, label):\n",
        "    # Apply random augmentations based on your requirements\n",
        "    # For example, you can resize, rotate, flip, change brightness, etc.\n",
        "    # Here, I'm applying random horizontal flip and rotation\n",
        "    augmented_image = image_data.copy()\n",
        "    if random.choice([True, False]):\n",
        "        augmented_image = np.fliplr(augmented_image)\n",
        "    rotation_angle = random.randint(-45, 45)\n",
        "    augmented_image = np.rot90(augmented_image, k=random.randint(0, 3), axes=(0, 1))\n",
        "\n",
        "    # Return the augmented image and its label\n",
        "    return augmented_image, label\n",
        "\n",
        "# Separate images and labels by class\n",
        "normal_images = [image_data for image_data, label in zip(normalized_images, labels) if label == 'Normal images']\n",
        "oscc_images = [image_data for image_data, label in zip(normalized_images, labels) if label == 'OSCC images']\n",
        "\n",
        "# Count the number of existing images for each class\n",
        "num_normal_images = len(normal_images)\n",
        "num_oscc_images = len(oscc_images)\n",
        "\n",
        "# Upsample the classes to match the number of synthetic images per class\n",
        "while num_normal_images < num_synthetic_images_per_class:\n",
        "    image_data = random.choice(normal_images)\n",
        "    synthetic_image, synthetic_label = generate_synthetic_image(image_data, 'Normal')\n",
        "    normal_images.append(synthetic_image)\n",
        "    num_normal_images += 1\n",
        "\n",
        "while num_oscc_images < num_synthetic_images_per_class:\n",
        "    image_data = random.choice(oscc_images)\n",
        "    synthetic_image, synthetic_label = generate_synthetic_image(image_data, 'OSCC')\n",
        "    oscc_images.append(synthetic_image)\n",
        "    num_oscc_images += 1\n",
        "\n",
        "\n",
        "# Sample 224 images randomly from each class\n",
        "selected_normal_images = random.sample(normal_images, num_synthetic_images_per_class)\n",
        "selected_oscc_images = random.sample(oscc_images, num_synthetic_images_per_class)\n",
        "\n",
        "# Combine images and labels to create the final dataset\n",
        "synthetic_images = (\n",
        "    selected_normal_images +\n",
        "    selected_oscc_images\n",
        ")\n",
        "\n",
        "synthetic_labels = (\n",
        "    ['Normal images'] * num_synthetic_images_per_class +\n",
        "    ['OSCC images'] * num_synthetic_images_per_class\n",
        ")\n",
        "\n",
        "print(\"Synthetic Images are generated by flipping & rotation, stored as synthetic_images and synthetic_labels\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeLCScyz_hKn"
      },
      "outputs": [],
      "source": [
        "#counting the number of labels in each classes\n",
        "\n",
        "count_of_classes = {}\n",
        "\n",
        "for label in synthetic_labels:\n",
        "    if label in count_of_classes:\n",
        "        count_of_classes[label] +=1\n",
        "    else:\n",
        "        count_of_classes[label] = 1\n",
        "\n",
        "\n",
        "for key, value in count_of_classes.items():\n",
        "    print(f'{key}:{value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inVt1Hv8_hKn"
      },
      "outputs": [],
      "source": [
        "# Function to display images\n",
        "def plot_images(original_images, generated_images, num_images=1000):\n",
        "    fig, axes = plt.subplots(num_images, 2, figsize=(8, 2*num_images))\n",
        "    for i in range(num_images):\n",
        "        axes[i, 0].imshow(original_images[i])\n",
        "        axes[i, 0].set_title('Original')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        axes[i, 1].imshow(generated_images[i])\n",
        "        axes[i, 1].set_title('Generated')\n",
        "        axes[i, 1].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display original and generated images side by side\n",
        "num_images_to_display = 5\n",
        "original_images_to_display = normalized_images[:num_images_to_display]\n",
        "generated_images_to_display = synthetic_images[:num_images_to_display]\n",
        "\n",
        "plot_images(original_images_to_display, generated_images_to_display, num_images=num_images_to_display)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM7kF9P7_hKn"
      },
      "source": [
        "#### Balanced number of images/class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyhp0RuJ_hKn"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iUpi8s6_hKo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize empty lists to store processed images and corresponding labels\n",
        "processed_images = []\n",
        "processed_labels = []\n",
        "\n",
        "# Load, preprocess, and align images and labels\n",
        "for image, label in zip(synthetic_images, synthetic_labels):\n",
        "    try:\n",
        "        processed_images.append(image)\n",
        "        processed_labels.append(label)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {str(e)}\")\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded_labels = label_encoder.fit_transform(processed_labels)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded_labels = integer_encoded_labels.reshape(len(integer_encoded_labels), 1)\n",
        "onehot_encoded_labels = onehot_encoder.fit_transform(integer_encoded_labels)\n",
        "\n",
        "# Convert processed_images and onehot_encoded_labels to numpy arrays\n",
        "processed_images = np.array(processed_images)\n",
        "onehot_encoded_labels = np.array(onehot_encoded_labels)\n",
        "\n",
        "# Save processed_images and onehot_encoded_labels in the current directory\n",
        "np.save(\"processed_images.npy\", processed_images)\n",
        "np.save(\"onehot_encoded_labels.npy\", onehot_encoded_labels)\n",
        "\n",
        "# Verify the shapes of processed_images and onehot_encoded_labels\n",
        "print(\"Shape of processed_images:\", processed_images.shape)\n",
        "print(\"Shape of onehot_encoded_labels:\", onehot_encoded_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-KsVdRL_hKo"
      },
      "source": [
        "# Model Development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHae5Ium_hKo"
      },
      "outputs": [],
      "source": [
        "# Import ResNeXt50 from tensorflow.keras.applications\n",
        "from tensorflow.keras.applications import VGG16, VGG19, ResNet50, InceptionV3, DenseNet121, MobileNetV2, Xception, InceptionResNetV2, EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMY34LmC_hKo"
      },
      "outputs": [],
      "source": [
        "#importing the model & dense layer for customizing the neural network\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kckFPnu4_hKo"
      },
      "outputs": [],
      "source": [
        "#input shape & dimension for the pre-trained models\n",
        "shape=(224, 224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ9_L5qE_hKo"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained models\n",
        "base_model_1 = VGG16(weights='imagenet', include_top=False, input_shape=shape)\n",
        "base_model_2 = VGG19(weights='imagenet', include_top=False, input_shape=shape)\n",
        "base_model_3 = ResNet50(weights='imagenet', include_top=False, input_shape=shape)\n",
        "base_model_4 = InceptionV3(weights='imagenet', include_top=False, input_shape=shape)\n",
        "base_model_5 = DenseNet121(weights='imagenet', include_top=False, input_shape=shape)\n",
        "base_model_6 = MobileNetV2(weights='imagenet', include_top=False, input_shape=shape)\n",
        "base_model_7 = Xception(weights='imagenet', include_top=False, input_shape=shape)\n",
        "base_model_8 = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=shape)\n",
        "#base_model_9 = EfficientNetB0(weights='imagenet', include_top=False, input_shape=shape)\n",
        "\n",
        "print('Loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOu-lab-_hKo"
      },
      "outputs": [],
      "source": [
        "base_models = [base_model_1, base_model_2, base_model_3,\n",
        "               base_model_4, base_model_5, base_model_6,\n",
        "               base_model_7, base_model_8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U75husMo_hKo"
      },
      "outputs": [],
      "source": [
        "# Looping through the Base models and printing the summaries\n",
        "for idx, model in enumerate(base_models):\n",
        "    print(f'Summary of Base Model {idx +1}:')\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyAvsOsc_hKo"
      },
      "outputs": [],
      "source": [
        "#Freezing the pre-trained model's last layer for transfer learning\n",
        "for model in base_models:\n",
        "    for layer in model.layers:\n",
        "        layer.trainable=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0TgX9VG_hKp"
      },
      "source": [
        "## Customize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yzx9HVJ_hKp"
      },
      "outputs": [],
      "source": [
        "custom_models = []\n",
        "\n",
        "for idx, model in enumerate(base_models):\n",
        "    x = model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    #x = Dense(512, activation='relu')(x)\n",
        "    predictions = Dense(2, activation='softmax')(x)\n",
        "    custom_model = Model(inputs=model.input, outputs=predictions)\n",
        "    custom_models.append(custom_model)\n",
        "    print(f\"Customized the model - {idx+1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RDuI3cT_hKp"
      },
      "outputs": [],
      "source": [
        "# Looping through the Base models and printing the summaries\n",
        "for idx, model in enumerate(custom_models):\n",
        "    print(f'Summary of Custom Model {idx +1}:')\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu0tfDPt_hKp"
      },
      "source": [
        "## Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqK4pKeI_hKp"
      },
      "outputs": [],
      "source": [
        "compiled_models = []\n",
        "for idx, custom_model in enumerate(custom_models):\n",
        "    custom_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    compiled_models.append(custom_model)\n",
        "    print(f\"Compiled Custom Model {idx + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JpvwEa8V_hKp"
      },
      "outputs": [],
      "source": [
        "for idx, model in enumerate(compiled_models):\n",
        "    print(f'Summary of compiled model {idx +1}: ')\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIX2vtTv_hKp"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky8dBJa9_hKq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80YV8OvM_hKq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and temporary sets (60% training, 40% temporary)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(processed_images, onehot_encoded_labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split the temporary data into testing and validation sets (50% testing, 50% validation)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Verify the shapes of the training, testing, and validation sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBNLyRf9_hKq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "\n",
        "def train_and_evaluate_models(X_train, y_train, X_test, y_test, compiled_models, model_names, epochs=100):\n",
        "    results = []\n",
        "    for model, model_name in zip(compiled_models, model_names):\n",
        "        # Initialize variables for tracking maximum accuracy and epoch\n",
        "        max_accuracy = 0\n",
        "        max_accuracy_epoch = 0\n",
        "\n",
        "        # Define a checkpoint to save the model when target accuracy is reached\n",
        "        checkpoint = ModelCheckpoint(f'{model_name}_model.h5', monitor='val_accuracy',\n",
        "                                     save_best_only=True, save_weights_only=False, mode='max', verbose=1)\n",
        "\n",
        "        # Define early stopping to stop training if accuracy doesn't improve for 10 epochs\n",
        "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "\n",
        "        # Define CSV logger to save training history\n",
        "        csv_logger = CSVLogger(f'{model_name}_training_history.csv', append=True)\n",
        "\n",
        "        # Train the current model\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_test, y_test),\n",
        "            batch_size=32,\n",
        "            callbacks=[checkpoint, early_stopping, csv_logger]\n",
        "        )\n",
        "\n",
        "        # Get the epoch at which maximum accuracy is obtained\n",
        "        max_accuracy_epoch = np.argmax(history.history['val_accuracy'])\n",
        "        max_accuracy = history.history['val_accuracy'][max_accuracy_epoch]\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "        print(f'Test Accuracy for {model_name}: {test_accuracy}')\n",
        "\n",
        "        # Store results including the epoch\n",
        "        results.append({'Model': model_name, 'Accuracy': max_accuracy, 'Epoch': max_accuracy_epoch+1})\n",
        "\n",
        "        print(\"-\" * 40)  # Print a line of dashes\n",
        "        print(f\"Maximum accuracy of {max_accuracy:.2f} achieved at epoch {max_accuracy_epoch+1}\")\n",
        "        print(\"Model with high accuracy is saved using the keras ModelCheckpoint\")\n",
        "        print(\"-\" * 40)  # Print a line of dashes\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # Assuming model.predict returns probabilities for each class\n",
        "        y_pred_probs = model.predict(X_test)\n",
        "\n",
        "        # Convert probabilities to class labels\n",
        "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "        # Convert true labels to class labels if y_test is one-hot encoded\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Generate confusion matrix\n",
        "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.title(model_name)\n",
        "        plt.show()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQvs8NiW_hKq"
      },
      "outputs": [],
      "source": [
        "print(count_of_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RjQxXE9_hKr"
      },
      "outputs": [],
      "source": [
        "model_names = [\"VGG16\", \"VGG19\", \"ResNet50\", \"InceptionV3\", \"DenseNet121\", \"MobileNetV2\", \"Xception\", \"InceptionResNetV2\"]\n",
        "class_labels = ['Normal', 'OSCC']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "S_uDamao_hKr"
      },
      "outputs": [],
      "source": [
        "#Train and Evaluate the model\n",
        "results = train_and_evaluate_models(X_train, y_train, X_test, y_test, compiled_models, model_names, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9sMKYT__hKr"
      },
      "outputs": [],
      "source": [
        "print(results) #print the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn_DTTGa_hKr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bYYZppa_hKr"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Define the headers for the table\n",
        "headers = [\"Model\", \"Epoch\"]\n",
        "\n",
        "# Print the table\n",
        "print('Models Accuracy')\n",
        "print(tabulate(results_df, headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXAYwCI_hKr"
      },
      "outputs": [],
      "source": [
        "#prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPMis2Zs_hKr"
      },
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8TbwH3-_hKs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "directory_path = '/tf/red-model/added models'\n",
        "loaded_models = {}\n",
        "\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".h5\"):  # Assuming the models are saved in HDF5 format with .h5 extension\n",
        "        model_name = os.path.splitext(filename)[0]  # Get the model name without extension\n",
        "        model_path = os.path.join(directory_path, filename)\n",
        "        loaded_model = tf.keras.models.load_model(model_path)  # Load the model using TensorFlow\n",
        "        loaded_models[model_name] = loaded_model\n",
        "\n",
        "# Print the loaded model names\n",
        "print(\"Loaded model names:\")\n",
        "print(list(loaded_models.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnQCxlQ9_hKs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "for model_name, model in loaded_models.items():\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate accuracy score\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(y_true, y_pred, target_names=class_labels)\n",
        "\n",
        "    # Print individual class prediction accuracy score\n",
        "    print(f\"Accuracy for {model_name}: {accuracy:.2f}\")\n",
        "    print(f\"Classification Report for {model_name}:\\n{class_report}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "ap7VIZkt_hKs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for model_name, model in loaded_models.items():\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate accuracy score\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f\"{model_name} - Accuracy: {accuracy:.2f}\")  # Include accuracy in the title\n",
        "    plt.show()\n",
        "\n",
        "    # Print accuracy score\n",
        "    print(f\"Accuracy for {model_name}: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuCAiAY4_hKs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "for model_name, model in loaded_models.items():\n",
        "    y_pred_probs = model.predict(X_val)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_val, axis=1)\n",
        "\n",
        "    # Calculate accuracy score\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(y_true, y_pred, target_names=class_labels)\n",
        "\n",
        "    # Print individual class prediction accuracy score\n",
        "    print(f\"Accuracy for {model_name}: {accuracy:.2f}\")\n",
        "    print(f\"Classification Report for {model_name}:\\n{class_report}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-WGvsJa_hKs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for model_name, model in loaded_models.items():\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate accuracy score\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f\"{model_name} - Accuracy: {accuracy:.2f}\")  # Include accuracy in the title\n",
        "    plt.show()\n",
        "\n",
        "    # Print accuracy score\n",
        "    print(f\"Accuracy for {model_name}: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU8Tg0S5_hKt"
      },
      "outputs": [],
      "source": [
        "# test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kyYq6vu_hKt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.models import load_model\n",
        "\n",
        "# Define class labels and their corresponding encoded values\n",
        "\n",
        "# Load the pre-trained models\n",
        "model_folder = '/tf/Purushothaman/6 Class model/Balanced Model'\n",
        "# Replace with the path to your model folder\n",
        "model_files = os.listdir(model_folder)\n",
        "loaded_models = {}\n",
        "\n",
        "for model_file in model_files:\n",
        "    try:\n",
        "        model_name, extension = os.path.splitext(model_file)\n",
        "        if extension == '.h5':\n",
        "            model_path = os.path.join(model_folder, model_file)\n",
        "            loaded_models[model_name] = load_model(model_path)\n",
        "            print(f\"Loaded model: {model_name}\")\n",
        "        else:\n",
        "            print(f\"Skipped file: {model_file} (not a valid model file)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model {model_file}: {str(e)}\")\n",
        "\n",
        "print(\"Model loading completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uci2WF7U_hKt"
      },
      "outputs": [],
      "source": [
        "# Load label encoder classes\n",
        "label_encoder_classes = np.load(os.path.join(model_folder, 'onehot_encoded_labels.npy'))\n",
        "\n",
        "# Set the classes_ attribute of the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.classes_ = label_encoder_classes\n",
        "print('loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYon35_n_hKt"
      },
      "outputs": [],
      "source": [
        "class_labels = ['Normal images', 'OSCC images']\n",
        "label_encoder.fit(class_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "OhoN26ZL_hKt"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define the folder containing images\n",
        "image_folder = \"/tf/Test images/\"\n",
        "\n",
        "# List all files in the folder\n",
        "image_files = os.listdir(image_folder)\n",
        "\n",
        "# Define the target size for resizing\n",
        "target_size = (224, 224)\n",
        "\n",
        "# Define a list to store the results\n",
        "image_results = []\n",
        "\n",
        "# Iterate through each image file\n",
        "for image_file in image_files:\n",
        "    # Get the full file path\n",
        "    image_path = os.path.join(image_folder, image_file)\n",
        "\n",
        "    try:\n",
        "        # Open and process the image using PIL (Pillow)\n",
        "        pil_image = Image.open(image_path)\n",
        "\n",
        "        # Resize and convert image to RGB format if necessary\n",
        "        pil_image = pil_image.resize(target_size)\n",
        "        if pil_image.mode != 'RGB':\n",
        "            pil_image = pil_image.convert('RGB')\n",
        "\n",
        "        # Convert image to numpy array\n",
        "        numpy_image = np.array(pil_image)\n",
        "\n",
        "        # Normalize image to [0, 1]\n",
        "        normalized_image = numpy_image.astype(np.float32) / 255.0\n",
        "\n",
        "        # Add batch dimension\n",
        "        normalized_image = np.expand_dims(normalized_image, axis=0)\n",
        "\n",
        "        image_result = {\"Image\": image_file}\n",
        "\n",
        "        # Iterate through loaded models and make predictions\n",
        "        for model_name, model in loaded_models.items():\n",
        "            # Predict using the current model\n",
        "            predictions = model.predict(normalized_image)\n",
        "\n",
        "            # Convert numerical predictions to class labels using inverse_transform\n",
        "            predicted_classes = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "            # Add model predictions to the dictionary\n",
        "            image_result[model_name] = predicted_classes[0]  # Store decoded predictions for each model\n",
        "\n",
        "        # Add the dictionary to the results list\n",
        "        image_results.append(image_result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_file}: {str(e)}\")\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "df = pd.DataFrame(image_results)\n",
        "\n",
        "# Print the DataFrame with model names as columns\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tQrlFi6_hKt"
      },
      "outputs": [],
      "source": [
        "# Export the DataFrame to a CSV file\n",
        "df.to_csv('output.csv', index=False)\n",
        "\n",
        "print(\"DataFrame has been exported to 'output.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0-Ibl6h_hKu"
      },
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_ef-bYj_hKu"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/tf/Purushothaman/6 Class model/Balanced Model/output.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_nbIGu8_hKu"
      },
      "outputs": [],
      "source": [
        "df.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW-TXkYu_hKu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}